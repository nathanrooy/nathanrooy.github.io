<!doctype html><html lang=en><head><meta charset=utf-8><title>Nathan Rooy</title>
<meta content="Nathan Rooy" name=author><meta content="en" name=language><meta content="width=device-width,minimum-scale=1" name=viewport><meta content="nry.me" property="og:site_name"><meta content="nry.me" property="twitter:domain"><meta content="article" property="og:type"><meta content="/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/" property="og:url"><meta content="/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/" name=twitter:url><meta content="Nathan A. Rooy" name=copyright><meta content="summary_large_image" name=twitter:card><meta content="python,machine learning,natural language processing" name=keywords><meta content="Word2vec from Scratch with Python and NumPy" name=twitter:title><meta content="Word2vec from Scratch with Python and NumPy" property="og:title"><meta content="A Python tutorial where I cover the word2vec skip-gram model and implement a barebones version utilizing NumPy" name=description><meta content="A Python tutorial where I cover the word2vec skip-gram model and implement a barebones version utilizing NumPy" name=twitter:description><meta content="A Python tutorial where I cover the word2vec skip-gram model and implement a barebones version utilizing NumPy" property="og:description"><meta content="/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/skip-gram-architecture.png" name=twitter:image><meta content="/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/skip-gram-architecture.png" property="og:image"><meta content="/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/skip-gram-architecture.png" name=thumbnail><link href=https://nry.me/index.xml rel=alternate type=application/rss+xml title="Nathan Rooy"><link href=/favicon/favicon.png rel=icon type=image/png><link href=/fonts/figtree-v1-latin-regular.woff2 rel=preload as=font type=font/woff2 crossorigin><link href=/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/ rel=canonical><link rel=blogroll type=text/xml href=https://raw.githubusercontent.com/nathanrooy/feedly-opml-archiver/refs/heads/main/feedly.opml><script defer data-domain=nry.me src=https://plausible.io/js/script.tagged-events.js></script><style>:root{--main-text-color:#222;--main-text-weight:400;--text-bold:600;--font-family:"Figtree", sans-serif;--background-light:#F5F5F5;--link-text-color:#0955b5;--dark-text-color:#999}html{font-size:16px}body{-webkit-font-smoothing:antialiased;font-family:var(--font-family);color:var(--main-text-color);font-weight:var(--main-text-weight)}.wrapper{max-width:660px;margin:6rem auto 0;padding:1rem}@media only screen and (max-width:750px){.wrapper{margin-top:2.5rem;margin-left:0}}a{text-decoration:none;color:var(--link-text-color)}a:hover{text-decoration:underline}a:visited{color:var(--link-text-color)}::selection{background:#fff9c4;text-shadow:none}img{margin:1.5em auto;max-width:100%;display:block}a img{border:none}@font-face{font-family:Figtree;font-style:normal;font-weight:400;font-display:swap;src:url(/fonts/figtree-v1-latin-regular.eot);src:local(''),url(/fonts/figtree-v1-latin-regular.eot?#iefix)format('embedded-opentype'),url(/fonts/figtree-v1-latin-regular.woff2)format('woff2'),url(/fonts/figtree-v1-latin-regular.woff)format('woff'),url(/fonts/figtree-v1-latin-regular.ttf)format('truetype'),url(/fonts/figtree-v1-latin-regular.svg#Figtree)format('svg')}@font-face{font-family:Figtree;font-style:normal;font-weight:600;font-display:swap;src:url(/fonts/figtree-v1-latin-600.eot);src:local(''),url(/fonts/figtree-v1-latin-600.eot?#iefix)format('embedded-opentype'),url(/fonts/figtree-v1-latin-600.woff2)format('woff2'),url(/fonts/figtree-v1-latin-600.woff)format('woff'),url(/fonts/figtree-v1-latin-600.ttf)format('truetype'),url(/fonts/figtree-v1-latin-600.svg#Figtree)format('svg')}.wrapper{padding-bottom:8rem}.markdown ol li{padding-bottom:.75em}p{font-size:1rem;margin:0 0 1rem;line-height:1.7rem;letter-spacing:.01rem;word-spacing:.025rem}h1{font-weight:var(--main-text-weight);font-size:2.3rem;color:var(--main-text-color);line-height:2.64rem}h2{font-size:1.2rem;font-weight:var(--text-bold);margin-bottom:.3em}*+h2{padding-top:2rem}table{width:100%;font-size:14px;letter-spacing:-.01em;border-collapse:collapse;margin:2rem auto;line-height:1.2rem}table th{text-align:left}table thead{border-bottom:1px solid var(--main-text-color);line-height:1.4em}ul{padding-left:15px;list-style:disc inside}ul li{margin-bottom:.5rem}ul li p{display:inline}hr{margin:4rem 0;height:1px;border:none;color:var(--main-text-color);background-color:var(--main-text-color)}@media only screen and (min-width:751px){#burger{padding-left:2rem;padding-top:2rem}}@media only screen and (max-width:750px){#burger{padding-left:1rem;padding-top:1rem}}#burger{font-family:var(--font-family);font-weight:400;color:#a9a9a9;font-size:1.5rem}#burger a{display:inline-block;padding-right:.8rem}#burger label{cursor:pointer}#burger label:hover{color:var(--main-text-color)}#nav-icons{padding-left:.8rem}#nav-icons svg{max-height:20px;margin-bottom:-.05rem;color:var(--main-text-color)}#l1-val{position:relative;top:-.12rem}#l2-val{position:relative;top:-.2rem;padding-left:.1rem;padding-right:.1rem}#burger-cbox{display:none}#label-1{display:inline-block}#label-2{display:none}#burger-cbox:not(:checked)~#nav-icons{display:none}#burger-cbox:checked~#nav-icons{display:inline-block}#burger-cbox:checked~#label-1{display:none}#burger-cbox:checked~#label-2{display:inline-block;color:var(--main-text-color)}.TLDR{font-size:1.1rem}figure{margin:0;text-align:center}.figcaption{font-size:.8rem;padding-bottom:1rem}.post .post-header{margin-bottom:2.5rem}.post .post-header .title{margin:0}.post blockquote{background-color:var(--background-light);border-left:10px solid;border-color:#2dad60;padding:1.5rem;margin:0 0 1rem;line-height:1.5rem;font-size:15px}.highlighter-rouge{position:relative;font-size:.75rem;color:var(--main-text-color);border:1px solid;border-color:#e6e6e6;border-radius:3px;background-color:var(--background-light);padding:.1rem .2rem}.math-block{margin-top:2rem;margin-bottom:2rem}mtext{position:absolute;right:0}.inline-svg-equation{text-align:center;margin:1.5em auto;max-width:100%;display:block}.highlight pre{padding:20px;background-color:transparent!important;overflow:auto;word-wrap:normal;white-space:pre}.highlight{background:0 0;background-color:var(--background-light);margin:1.5rem auto;font-size:.85rem}.markdown .footnotes ol{counter-reset:item;padding-left:0;padding-right:0;width:fit-content;color:var(--dark-text-color)}.markdown .footnotes li{display:block;padding-bottom:0}.markdown .footnotes li::before{content:counter(item)". ";counter-increment:item;width:2em;display:inline-block;position:absolute}.markdown .footnotes li p{padding-left:2rem;line-height:1.25rem}.markdown .footnotes li a{color:var(--dark-text-color)}.markdown .footnotes li a:hover{color:var(--main-text-color)}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css integrity=sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+ crossorigin=anonymous></head><body><div class=header><nav><div id=burger><input type=checkbox id=burger-cbox>
<label id=label-1 for=burger-cbox><span>[</span><span id=l1-val>+</span><span>]</span>
</label><label id=label-2 for=burger-cbox><span>[</span><span id=l2-val>x</span><span>]</span></label><div id=nav-icons><a class="plausible-event-name=burger+home" aria-label=Home style=padding-right:.7rem href=/><svg id="home-icon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg></a>
<a class="plausible-event-name=burger+reading" aria-label=Reading href=/reading><svg width="24" height="24" viewBox="0 0 23.809855 18.892912" id="svg5"><defs id="defs2"/><g id="layer1" transform="translate(-6.352941,-30.26865)"><g id="g39" transform="matrix(0.26458333,0,0,0.26458333,5.0300244,26.48677)"><path class="st0" d="m92.22 22.9v59C74.24 76.35 61.26 78.85 54.73 81.04 59.2 76.64 68.54 70.55 84.16 74.14l1.69.39V21.42c2.17.29 4.3.77 6.37 1.48z" id="path11"/><path class="st0" d="M45.24 81.04C38.73 78.85 25.74 76.35 7.77 81.9v-59c2.06-.71 4.2-1.19 6.37-1.48v53.11l1.68-.39c15.61-3.58 24.96 2.5 29.42 6.9z" id="path13"/><polygon points="48.86,85.63 48.82,85.58 48.91,85.63" id="polygon15"/><polygon points="51.17,85.58 51.13,85.63 50,85.63 51.09,85.62" id="polygon17"/><path d="m94.09 20.63c-2.68-1-5.45-1.65-8.24-2.02v-2.94l-1.08-.24c-17.8-4.09-28.32 3.81-32.9 8.68-.81.87-1.44 1.63-1.88 2.22C49.54 25.74 48.91 24.98 48.1 24.11c-4.58-4.87-15.11-12.76-32.9-8.68l-1.07.24v2.94c-2.8.38-5.57 1.02-8.24 2.02L5 20.97V85.7l1.81-.61c13.49-4.49 24.13-4.16 30.68-3.08 7.09 1.15 11.05 3.4 11.09 3.42l.23.14.09.06H50l1.09-.01.08-.05.23-.14c.05-.02 4-2.27 11.09-3.42 6.55-1.08 17.19-1.41 30.68 3.08l1.82.61V20.97zM7.78 81.89v-59c2.06-.71 4.2-1.19 6.37-1.48v53.11l1.68-.39c15.61-3.58 24.95 2.5 29.42 6.9-6.52-2.18-19.5-4.68-37.47.86zm40.76-1.3c-4.19-4.66-14.29-12.83-31.63-9.5v-53.2c11.11-2.28 18.93.38 24.02 3.66 2.6 1.68 4.48 3.52 5.71 4.95 1.14 1.32 1.73 2.28 1.82 2.43l.08.15zm2.76.17V29.34l.24-.4c.09-.16.68-1.12 1.82-2.44 1.23-1.43 3.11-3.27 5.71-4.94 5.08-3.29 12.91-5.95 24.02-3.67v53.2C65.54 67.72 55.4 76.13 51.3 80.76zm40.92 1.13C74.24 76.34 61.26 78.84 54.73 81.03 59.2 76.63 68.54 70.54 84.16 74.13l1.69.39v-53.1c2.16.29 4.3.77 6.37 1.48z" id="path19"/></g></g><style id="style9">.st0{fill:none}</style></svg>
</a><a class="plausible-event-name=burger+nycbike" aria-label="NYC Bike Exploring" style=margin-left:.4rem href=/bike-exploring/nyc><svg viewBox="0 0 91.5 52.5" width="32" height="24"><g transform="matrix(-1,0,0,1,95.8,-23.9)"><path d="m78 40.6c-3.8.0-7.3 1.2-10.2 3.2l-7.4-9.4 2.9-8.5H67c.6.0 1-.4 1-1s-.4-1-1-1H54.8c-.6.0-1 .4-1 1s.4 1 1 1h6.3l-2.3 6.7c-.2-.2-.4-.3-.7-.3H32l2.2-6.4h5.7c.6.0 1-.4 1-1s-.4-1-1-1h-6.4c-.4.0-.8.3-.9.7l-5.7 16.8c-1.5-.4-3.1-.7-4.8-.7-9.8.0-17.8 8-17.8 17.8s8 17.8 17.8 17.8 17.8-8 17.8-17.8c0-7.5-4.6-13.9-11.2-16.5l2.6-7.6 19 24.4c.1.3.3.6.6.7.1.0.2.1.3.1h.1 8.9c.5 9.4 8.3 16.8 17.8 16.8 9.8.0 17.8-8 17.8-17.8s-8-18-17.8-18zM37.8 58.4c0 8.7-7.1 15.8-15.8 15.8S6.2 67.1 6.2 58.4 13.3 42.6 22 42.6c1.4.0 2.8.2 4.1.6L21 58.1c-.2.5.1 1.1.6 1.3.1.0.2.1.3.1.4.0.8-.3.9-.7l5.1-14.9c5.9 2.2 9.9 7.9 9.9 14.5zm29.7-11.8L76 57.4H62.2c.3-4.3 2.2-8.1 5.3-10.8zm-16.7 9.6-17.1-22h24l.4.5zm1.8 1.2 7.1-20.8 6.6 8.4c-3.5 3.1-5.7 7.4-6 12.4zM78 74.2c-8.4.0-15.2-6.6-15.8-14.8H78c.4.0.7-.2.9-.6.2-.3.1-.8-.1-1.1L69 45.4c2.5-1.8 5.6-2.8 8.9-2.8 8.7.0 15.8 7.1 15.8 15.8s-7 15.8-15.7 15.8z"/></g></svg></a>
<a class="plausible-event-name=burger+about" aria-label=About href=/about><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user" id="about-icon"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
</a><a class="plausible-event-name=burger+rss" aria-label=RSS href=/index.xml><svg width="24" height="24" viewBox="0 0 30 24" fill="none" stroke="#444" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss" id="rss-icon"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></div></div></nav></div><div class="wrapper post"><div class=post-header><h1 class=title>Word2vec from Scratch with Python and NumPy</h1></div><p class=TLDR><strong>TL;DR</strong> - word2vec is awesome, it's also really simple. Learn how it works, and implement your own version.</p><hr><div class=markdown><h2>Introduction</h2><p>Since joining a tech startup back in 2016, my life has revolved around machine learning and natural language processing (NLP). Trying to extract faint signals from terabytes of streaming social media is the name of the game. Because of this, I&rsquo;m constantly experimenting and implementing different NLP schemes; word2vec being among the simplest and coincidently yielding great predictive value. The underpinnings of word2vec are exceptionally simple and the math is borderline elegant. The whole system is deceptively simple, and provides exceptional results. This tutorial aims to teach the basics of word2vec while building a barebones implementation in <a target=_blank rel="noopener noreferrer" href=https://www.python.org/>Python</a> using <a target=_blank rel="noopener noreferrer" href=http://www.numpy.org/>NumPy</a>. Note that the final Python implementation will not be optimized for speed or memory usage, but instead for easy understanding.</p><p>The goal with word2vec and most NLP embedding schemes is to translate text into vectors so that they can then be processed using operations from linear algebra. Vectorizing text data allows us to then create predictive models that use these vectors as input to then perform something useful. If you understand both forward and back propagation for plain <a target=_blank rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Multilayer_perceptron>vanella neural networks</a>, you already understand 90% of word2vec.</p><h2>It's Actually Really Simple (I promise!)</h2><p>Word2vec is actually a collection of two different methods: continuous bag-of-words (CBOW) and skip-gram<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Given a word in a sentence, lets call it w(t) (also called the <i>center word</i> or <i>target word</i>), CBOW uses the <i>context</i> or surrounding words as input. For instance, if the context window <i>C</i> is set to C=5, then the input would be words at positions w(t-2), w(t-1), w(t+1), and w(t+2). Basically the two words before and after the center word w(t). Given this information, CBOW then tries to predict the target word.</p><img alt="word2vec network architecture" title="word2vec network architecture" src=word2vec_network_architecture.png><center class=figcaption>Figure 1: word2vec CBOW and skip-gram network architectures</center><p>The second method, skip-gram is the exact opposite. Instead of inputting the context words and predicting the center word, we feed in the center word and predict the context words. This means that w(t) becomes the input while w(t-2), w(t-1), w(t+1), and w(t+2) are the ideal output. For for this post, we&rsquo;re only going to consider the skip-gram model since it has been shown to produce better word-embeddings than CBOW.</p><p>The concept of a center word surrounded by context words can be likened to a sliding window that travels across the text corpus. As an example, lets encode the following sentence: <i>&ldquo;the quick brown fox jumps over the lazy dog&rdquo;</i> using a window size of C=5 (two before, and two after the center word). As the context window slides across the sentence from left to right, it gets populated with the corresponding words. When the context window reaches the edges of the sentences, it simply drops the furthest window positions. Below is what this process looks like. Note that instead of w(t), w(t+1), etc., the center word has become x<sub>k</sub> and the context words have become y<sub>c</sub>.</p><img alt="text encoding" title="text encoding" src=text-example.png><center class=figcaption>Figure 2: a sliding window example</center><p>Because we can&rsquo;t send text data directly through a matrix, we need to employ <a target=blank href=https://en.wikipedia.org/wiki/One-hot>one-hot encoding</a>. This means we have a vector of length <i>v</i> where v is the total number of unique words in the text corpus (or shorter if we want). Each word corresponds to a single position in this vector, so when embedding the word v_n, everywhere in vector v is zero except v_n which becomes a one. Below in Figure 3, a one-hot encoding of examples 1, 5, and 9 from Figure 2 above.</p><img alt="one-hot encoding" id=figure-3 title="one-hot encoding" src=training-data.png><center class=figcaption>Figure 3: one-hot encoding</center><p>→ For future reference, the column vectors <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>c</mi><mo>=</mo><mi>C</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> y_{c=1},...,y_{c=C} </annotation></semantics></math></span> are referred to as <b><i>panels</i></b>.</p><p>So far, so good right? Now we need to feed this data into the network and train it. Most of the literature describing skip-gram uses the same graphic depicting the final layer of the model somehow having three or more matrices. I found this rather confusing while I was reading the white papers so I ended up digging through the <a target=_blank rel="noopener noreferrer" href=https://github.com/tmikolov/word2vec>original source code</a> to get to the bottom of it. I figure there are other people like me, so I created another version of the architecture that I find a little easier to digest.</p><img id=figure-4 alt="skip-gram network architecture" title="skip-gram network architecture" src=skip-gram-architecture.png><center class=figcaption>Figure 4: skip-gram network architecture</center><h2>Forward Pass</h2><p>Now that the text data has been encoded, lets proceed through a single forward pass of the network. Step one is getting to the hidden layer <i>h</i>.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>W</mi></mrow><annotation encoding="application/x-tex"> h = x^T W </annotation></semantics></math></span></div><p>Since <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span> is a one-hot encoded vector, <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex"> h </annotation></semantics></math></span> is simply the <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>k</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex"> k^{th} </annotation></semantics></math></span> row of matrix <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span>.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo>=</mo><msubsup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">)</mo></mrow><mi>T</mi></msubsup><mo>=</mo><msubsup><mi>v</mi><msub><mi>w</mi><mi>I</mi></msub><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex"> h = W_{(k,:)}^T = v_{w_I}^T </annotation></semantics></math></span></div><p>Preceding forward, we need to calculate the values as they exit the network.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>u</mi><mi>c</mi></msub><mo>=</mo><msup><mi>W</mi><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msup><mi>h</mi><mo>=</mo><msup><mi>W</mi><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msup><msup><mi>W</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex"> u_c = W^{\prime T}  h = W^{\prime T} W^T x </annotation></semantics></math></span></div><p>Although <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex"> u_c </annotation></semantics></math></span> gets us through the network, we need to apply a <a target=_blank rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Softmax_function>softmax</a> function. The softmax will compress each element of <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex"> u_c </annotation></semantics></math></span> to a range of [0,1] while also forcing the sum of <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex"> u_c </annotation></semantics></math></span> to equal one. This will help in computing network error and backpropagation later on.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>y</mi><mi>c</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow><mi>O</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>I</mi></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>y</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><msup><mi>j</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>u</mi><msup><mi>j</mi><mo mathvariant="normal">′</mo></msup></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
y_c &amp; = Softmax(u) \\[1.5em]
p \left( w_{c,j} = w_{O,c} | w_I \right) &amp; = y_{c,j} = \frac{exp(u_{c,j})}{\sum_{j^\prime=1}^V exp(u_{j^\prime})}
\end{aligned}
</annotation></semantics></math></span></div><p>For each one-hot encoded center word <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span>, matrix <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^\prime </annotation></semantics></math></span> will output a certain set of values. This means that all the panels associated with center word <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span> share the same input values, thus:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>u</mi><mi>j</mi></msub><mo>=</mo><msup><msubsup><mi>v</mi><msub><mi>w</mi><mi>j</mi></msub><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mi>T</mi></msup><mo>⋅</mo><mi>h</mi><mspace width="2em"/><mtext>for</mtext><mtext>  </mtext><mi>c</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex"> u_{c,j} = u_j = {v_{w_j}^{\prime}}^T \cdot h \qquad \text{for} \; c = 1,2,...,C </annotation></semantics></math></span></div><p>where <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><msub><mi>w</mi><mi>j</mi></msub><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex"> v_{w_j}^{\prime} </annotation></semantics></math></span> is the output vector of the j-th word in the vocabulary, <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> w_j </annotation></semantics></math></span>. Again because we&rsquo;re dealing with one-hot encodings, this means <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><msub><mi>w</mi><mi>j</mi></msub><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex"> v_{w_j}^{\prime} </annotation></semantics></math></span> is a column vector taken from the hidden → output matrix <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^{\prime} </annotation></semantics></math></span>. Refer to <a href=#figure-4>figure-4</a> for clarity.</p><p>In code, this will take the form of the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># FORWARD PASS</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward_pass</span>(self, x):
</span></span><span style=display:flex><span>    h <span style=color:#555>=</span> np<span style=color:#555>.</span>dot(self<span style=color:#555>.</span>w1<span style=color:#555>.</span>T, x)
</span></span><span style=display:flex><span>    u_c <span style=color:#555>=</span> np<span style=color:#555>.</span>dot(self<span style=color:#555>.</span>w2<span style=color:#555>.</span>T, h)
</span></span><span style=display:flex><span>    y_c <span style=color:#555>=</span> self<span style=color:#555>.</span>softmax(u)
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>return</span> y_c, h, u
</span></span></code></pre></div><p>With the softmax calculation taking the form of:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># SOFTMAX ACTIVATION FUNCTION</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>softmax</span>(self, x):
</span></span><span style=display:flex><span>    e_x <span style=color:#555>=</span> np<span style=color:#555>.</span>exp(x <span style=color:#555>-</span> np<span style=color:#555>.</span>max(x))
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>return</span> e_x <span style=color:#555>/</span> e_x<span style=color:#555>.</span>sum(axis<span style=color:#555>=</span><span style=color:#f60>0</span>)
</span></span></code></pre></div><p>The forward pass is fairly simple and differs little from that of a standard, fully connected neural network.</p><h2>Backpropagation & Training</h2><p>Now to improve the weights within <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span> and <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^{\prime} </annotation></semantics></math></span> we&rsquo;re going to use <a target=_blank rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Stochastic_gradient_descent>stochastic gradient decent</a> (SGD) to <a target=_blank rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Backpropagation>backpropagate</a> the errors, which means we need to calculate the loss on the output layer.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi>E</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi mathvariant="double-struck">P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>O</mi><mo separator="true">,</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>O</mi><mo separator="true">,</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>O</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>I</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><munderover><mo>∏</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><msubsup><mi>j</mi><mi>c</mi><mo>∗</mo></msubsup></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>u</mi><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><msub><mi>u</mi><msubsup><mi>j</mi><mi>c</mi><mo>∗</mo></msubsup></msub><mo>+</mo><mi>C</mi><mo>⋅</mo><mi>log</mi><mo>⁡</mo><munderover><mo>∑</mo><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>u</mi><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex"> 
\begin{aligned}
E &amp; = - \log \mathbb P ( w_{O,1}, w_{O,2},...,w_{O,c} | w_I ) \\[1.5em]
&amp; = -\log \prod_{c=1}^C \frac{ \exp(u_{c,j_c^*})}{\sum_{j^{\prime}=1}^V \exp(u_{j^{\prime}})} \\[1.5em]
&amp; = - \sum_{c=1}^C u_{j_c^*} + C \cdot \log \sum_{j^{\prime}=1}^V \exp(u_{j^{\prime}})
\end{aligned}
</annotation></semantics></math></span></div><p>where <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>j</mi><mi>c</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex"> j_c^* </annotation></semantics></math></span> represents the index of the actual c-th output context word in the vocabulary.</p><p>Now that the loss has been calculated, we&rsquo;re going to employ the <a target=_blank rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Chain_rule>chain rule</a> to distribute the error amongst the weight matrices <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span> and <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^{\prime} </annotation></semantics></math></span>. First step is taking the derivative of <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex"> E </annotation></semantics></math></span> with respect to every element on every panel of the output layer <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> u_{c,j} </annotation></semantics></math></span>.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><msub><mi>y</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>t</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>:</mo><mo>=</mo><msub><mi>e</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> \frac{\partial E}{\partial u_{c,j}} = y_{c,j} - t_{c,j} := e_{c,j} </annotation></semantics></math></span></div><p>Where <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> t_{c,j} </annotation></semantics></math></span> is the ground truth for that particular panel. To simplify the notation going forward, we&rsquo;ll define the following:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><msub><mi>e</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mo fence="true">(</mo><msub><mi>y</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>t</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo fence="true">)</mo></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mi>j</mi></msub></mrow></mfrac></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex"> 
EI_j = \sum_{c=1}^C e_{c,j} = \sum_{c=1}^C \left( y_{c,j} - t_{c,j} \right) = \frac{\partial E}{\partial u_j} \tag{2}
</annotation></semantics></math></span></div><p>The column vector <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> EI_j </annotation></semantics></math></span> represents the row-wise sum of the prediction errors across each context word panel for the current center word. Proceeding backwards, we need to take the derivative of E with respect to <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^{\prime} </annotation></semantics></math></span> representing the output → hidden matrix.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msubsup><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mo fence="true">(</mo><msub><mi>y</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>t</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>⋅</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex"> 
\begin{aligned}
\frac{\partial E}{\partial w_{ij}^{\prime}} &amp; = \sum_{c=1}^C \frac{\partial E}{\partial u_{c,j}}  \cdot \frac{\partial u_{c,j}}{\partial w_{i,j}^{\prime}} \\[1.5em]
&amp; = \sum_{c=1}^C \left( y_{c,j} - t_{c,j} \right) \\[1.5em]
&amp; = EI_j \cdot h_i
\end{aligned}
</annotation></semantics></math></span></div><p>Therefore, the gradient decent update equation for <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex"> W^{\prime} </annotation></semantics></math></span> becomes:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mo mathvariant="normal">′</mo><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>w</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mo mathvariant="normal">′</mo><mo stretchy="false">(</mo><mi>o</mi><mi>l</mi><mi>d</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>η</mi><mo>⋅</mo><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>⋅</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> w_{i,j}^{\prime (new)} = w_{i,j}^{\prime (old)} - \eta \cdot EI_j \cdot h_i </annotation></semantics></math></span></div><p>Note that <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex"> \eta </annotation></semantics></math></span> is the learning rate. Next, lets formulate the error update equation for the input → hidden layer <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span> weights by deriving the error with respect to the hidden layer.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>h</mi><mi>i</mi></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mi>j</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>u</mi><mi>j</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>h</mi><mi>i</mi></msub></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>⋅</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal">′</mo></msubsup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex"> 
\begin{aligned}
\frac{\partial E}{\partial h_{i}} &amp; = \sum_{j=1}^V \frac{\partial E}{\partial u_j} \cdot \frac{\partial u_j}{\partial h_i} \\[1em]
&amp; = \sum_{j=1}^V EI_j \cdot w_{ij}^\prime \\
\end{aligned}
</annotation></semantics></math></span></div><p>This allows us to then calculate the loss with respect to <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span>.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>W</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>h</mi><mi>i</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>h</mi><mi>i</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>⋅</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>⋅</mo><msub><mi>x</mi><mi>k</mi></msub></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\frac{\partial E}{\partial W_{ki}} &amp; = \frac{\partial E}{\partial h_i} \cdot \frac{\partial h_i}{\partial w_{ki}} \\[1em]
 &amp; = \sum_{j=1}^V EI_j \cdot w_{ij}^{\prime} \cdot x_k
\end{aligned}
</annotation></semantics></math></span></div><p>and finally, we can formulate the gradient decent weight update equation for <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span>.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>w</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false">(</mo><mi>o</mi><mi>l</mi><mi>d</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>η</mi><mo>⋅</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>E</mi><msub><mi>I</mi><mi>j</mi></msub><mo>⋅</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>⋅</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> w_{ij}^{(new)} = w_{ij}^{(old)} - \eta \cdot \sum_{j=1}^V EI_j \cdot w_{ij}^{\prime} \cdot x_j </annotation></semantics></math></span></div><p>At this point, everything needed to train the network has been established and we just need to code it.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># TRAIN W2V model</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>train</span>(self, training_data):
</span></span><span style=display:flex><span>    <span style=color:#09f;font-style:italic># INITIALIZE WEIGHT MATRICES</span>
</span></span><span style=display:flex><span>    self<span style=color:#555>.</span>w1 <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>uniform(<span style=color:#555>-</span><span style=color:#f60>0.8</span>, <span style=color:#f60>0.8</span>, (self<span style=color:#555>.</span>v_count, self<span style=color:#555>.</span>n))     <span style=color:#09f;font-style:italic># context matrix</span>
</span></span><span style=display:flex><span>    self<span style=color:#555>.</span>w2 <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>uniform(<span style=color:#555>-</span><span style=color:#f60>0.8</span>, <span style=color:#f60>0.8</span>, (self<span style=color:#555>.</span>n, self<span style=color:#555>.</span>v_count))     <span style=color:#09f;font-style:italic># embedding matrix</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#09f;font-style:italic># CYCLE THROUGH EACH EPOCH</span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#366>range</span>(<span style=color:#f60>0</span>, self<span style=color:#555>.</span>epochs):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>loss <span style=color:#555>=</span> <span style=color:#f60>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#09f;font-style:italic># CYCLE THROUGH EACH TRAINING SAMPLE</span>
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> w_t, w_c <span style=color:#000;font-weight:700>in</span> training_data:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#09f;font-style:italic># FORWARD PASS</span>
</span></span><span style=display:flex><span>            y_pred, h, u <span style=color:#555>=</span> self<span style=color:#555>.</span>forward_pass(w_t)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#09f;font-style:italic># CALCULATE ERROR</span>
</span></span><span style=display:flex><span>            EI <span style=color:#555>=</span> np<span style=color:#555>.</span>sum([np<span style=color:#555>.</span>subtract(y_pred, word) <span style=color:#069;font-weight:700>for</span> word <span style=color:#000;font-weight:700>in</span> w_c], axis<span style=color:#555>=</span><span style=color:#f60>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#09f;font-style:italic># BACKPROPAGATION</span>
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>backprop(EI, h, w_t)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#09f;font-style:italic># CALCULATE LOSS</span>
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>loss <span style=color:#555>+=</span> <span style=color:#555>-</span>np<span style=color:#555>.</span>sum([u[word<span style=color:#555>.</span>index(<span style=color:#f60>1</span>)] <span style=color:#069;font-weight:700>for</span> word <span style=color:#000;font-weight:700>in</span> w_c]) <span style=color:#555>+</span> <span style=color:#366>len</span>(w_c) <span style=color:#555>*</span> np<span style=color:#555>.</span>log(np<span style=color:#555>.</span>sum(np<span style=color:#555>.</span>exp(u)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#366>print</span> <span style=color:#c30>&#39;EPOCH:&#39;</span>,i, <span style=color:#c30>&#39;LOSS:&#39;</span>, self<span style=color:#555>.</span>loss
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>pass</span>
</span></span></code></pre></div><p>Where the backpropagation function is defined as:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># BACKPROPAGATION</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backprop</span>(self, e, h, x):
</span></span><span style=display:flex><span>    dl_dw2 <span style=color:#555>=</span> np<span style=color:#555>.</span>outer(h, e)  
</span></span><span style=display:flex><span>    dl_dw1 <span style=color:#555>=</span> np<span style=color:#555>.</span>outer(x, np<span style=color:#555>.</span>dot(self<span style=color:#555>.</span>w2, e<span style=color:#555>.</span>T))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#09f;font-style:italic># UPDATE WEIGHTS</span>
</span></span><span style=display:flex><span>    self<span style=color:#555>.</span>w1 <span style=color:#555>=</span> self<span style=color:#555>.</span>w1 <span style=color:#555>-</span> (self<span style=color:#555>.</span>eta <span style=color:#555>*</span> dl_dw1)
</span></span><span style=display:flex><span>    self<span style=color:#555>.</span>w2 <span style=color:#555>=</span> self<span style=color:#555>.</span>w2 <span style=color:#555>-</span> (self<span style=color:#555>.</span>eta <span style=color:#555>*</span> dl_dw2)
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>pass</span>
</span></span></code></pre></div><p>That&rsquo;s it! Only slightly more complicated than a simple neural network. To avoid posting redundant sections of code, you can find the completed word2vec model along with some additional features at this GitHub repo (<a target=_blank rel="noopener noreferrer" href=https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py>link</a>).</p><h2>Results</h2><p>As a simple sanity check, lets look at the network output given a few input words. This is the output after 5000 iterations.</p><table class=results-table style="width:100%;border-spacing:20px 0;text-align:center"><thead><tr><th style="text-align:center;border-right:1px solid #000"><strong>Input word</strong></th><th style=text-align:center><strong>brown</strong></th><th style=text-align:center><strong>dog</strong></th><th style=text-align:center><strong>fox</strong></th><th style=text-align:center><strong>jumps</strong></th><th style=text-align:center><strong>lazy</strong></th><th style=text-align:center><strong>over</strong></th><th style=text-align:center><strong>quick</strong></th><th style=text-align:center><strong>the</strong></th></tr></thead><tbody style=font-size:11px;text-align:center><tr><td style="color:#9f1f63;font-size:14px;border-right:1px solid #000">fox</td><td style=background-color:#13a89e;color:#fff>2.45e-01</td><td>4.34e-04</td><td>4.45e-04</td><td style=background-color:#13a89e;color:#fff;margin-right:5px>2.53e-01</td><td>2.34e-05</td><td style=background-color:#13a89e;color:#fff>2.53e-01</td><td style=background-color:#13a89e;color:#fff>2.45e-01</td><td>7.62e-07</td></tr><tr><td style="color:#9f1f63;font-size:14px;border-right:1px solid #000">lazy</td><td>5.81e-05</td><td style=background-color:#13a89e;color:#fff>3.32e-01</td><td>2.42e-04</td><td>1.11e-05</td><td>1.91e-04</td><td style=background-color:#13a89e;color:#fff>3.33e-01</td><td>4.51e-04</td><td style=background-color:#13a89e;color:#fff>3.33e-01</td></tr><tr><td style="color:#9f1f63;font-size:14px;border-right:1px solid #000">dog</td><td>1.85e-07</td><td>3.17e-04</td><td>1.31e-03</td><td>1.29e-04</td><td style=background-color:#13a89e;color:#fff>4.98e-01</td><td>1.42e-05</td><td>4.86e-06</td><td style=background-color:#13a89e;color:#fff>4.99e-01</td></tr></tbody></table><p>These sample results show that the output probabilities for each center word are split fairly evenly between the correct context word. If we narrow in on the word <strong><i>lazy</i></strong>, we can see that the probabilities for the words <strong><i>dog</i></strong>, <strong><i>over</i></strong>, and <strong><i>the</i></strong> are split fairly evenly at roughly 33.33% each. This is exactly what we want.</p><h2>Improvements</h2><p>Shortly after the initial release of word2vec, a second paper detailing several improvements was published.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> Amongst these proposed improvements are:</p><p><strong>Phrase Generation</strong> — This is the process in which commonly co-occuring words such as &ldquo;san&rdquo; and &ldquo;francisco&rdquo; become &ldquo;san_francisco&rdquo;. The result of phrase generation is a cleaner, more useful, and user-friendly vocabulary list. Phrase generation is based on the following equation which utilizes the unigram and bigram vocabulary counts for the given corpus.</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>b</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>a</mi></msub><msub><mi>w</mi><mi>b</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>δ</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>a</mi></msub><mo stretchy="false">)</mo><mo>×</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> score(w_a, w_b) = \frac{count(w_aw_b) - \delta}{count(w_a) \times count(w_b)} </annotation></semantics></math></span></div><p>The numerator consists of the total number of times the bigram formed with words <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex"> w_a </annotation></semantics></math></span> and <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex"> w_b </annotation></semantics></math></span> appears in the corpus. This is then divided by the counts of <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex"> w_a </annotation></semantics></math></span> multiplied by <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex"> w_b </annotation></semantics></math></span>. The variable <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex"> \delta </annotation></semantics></math></span> is referred to as the <i>discounting coefficient</i> and prevents the formation of word phrases consisting of very infrequent words.<sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>For longer word combinations such as &ldquo;new york city&rdquo;, an iterative phrase generation approach can be used. As an example, on the initial pass &ldquo;new&rdquo; and &ldquo;york&rdquo; could be combined into &ldquo;new_york&rdquo;, with a second pass combining &ldquo;new_york&rdquo; with &ldquo;city&rdquo; yielding the desired phrase &ldquo;new_york_city&rdquo;. According to Mikolov et al. (<a href=#fn:fn1>2013</a>), typically 2-4 passes with decreasing threshold values yielded the best results.</p><p><strong>Subsampling</strong> — The purpose of subsampling is to counter the balance between frequent and rare words. No single word should represent a sizable portion of the corpus. To correct for this all words are discarded based on the following probability:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><msqrt><mfrac><mi>t</mi><mrow><mi>f</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow></mrow></mfrac></msqrt></mrow><annotation encoding="application/x-tex"> P \left( w_i \right) = 1 - \sqrt{\frac{t}{f \left( w_i \right)}} </annotation></semantics></math></span></div><p>Where <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> f(w_i) </annotation></semantics></math></span> is the frequency of word <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> w_i </annotation></semantics></math></span> and <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex"> t </annotation></semantics></math></span> is a user specified threshold. In Mikolov et al. (<a href=#fn:fn1>2013</a>), values for <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex"> t </annotation></semantics></math></span> were typically around 10<sup>-5</sup>. As pointed out <a target=_blank rel="noopener noreferrer" href=http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/>here</a>, this probability calculation differs from the <a target=_blank rel="noopener noreferrer" href=https://github.com/tmikolov/word2vec>official word2vec C implementation</a>. Below is the modified equation found in the C implementation:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">(</mo><msqrt><mfrac><mrow><mi>z</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>0.001</mn></mfrac></msqrt><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mfrac><mn>0.001</mn><mrow><mi>z</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> P(w_i) = \left( \sqrt{\frac{z(w_i)}{0.001}} + 1 \right) \times \frac{0.001}{z(w_i)} </annotation></semantics></math></span></div><p>Where <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> z(w_i) </annotation></semantics></math></span> is the fraction of the corpus represented by the word <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> w_i </annotation></semantics></math></span>. The higher <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> P(w_i) </annotation></semantics></math></span> is, the greather the chances are of keeping <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> w_i </annotation></semantics></math></span>.</p><p><strong>Negative Sampling</strong> — Often referred to as just NEG, this is a modification to the backpropagation procedure in which only a small percentage of the errors are actually considered. For instance, using example #9 from <a href=#figure-3>figure 3</a> above, <strong><i>dog</i></strong> is the target word, while <strong><i>the</i></strong> and <strong><i>lazy</i></strong> are the context words. This means that in an ideal situation, the network will return a &ldquo;0&rdquo; for everything except for <strong><i>the</i></strong> and <strong><i>lazy</i></strong> which will be &ldquo;1&rdquo;. In short, context words become &ldquo;positive&rdquo; while everything else becomes &ldquo;negative&rdquo;. With negative sampling, we only concern ourselves with the positive words and only a small percentage of the negative words. This means that instead of backpropagating the errors from all 8 words in the vocabulary, we backpropagate the errors from the positive words <strong><i>the</i></strong> and <strong><i>lazy</i></strong> plus <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex"> k </annotation></semantics></math></span> negative words. Because the vocabulary size in this example is only 8, the advantage of negative sampling may not be immediately apparent. Now, imagine for a moment that you have a corpus that yielded billions of training samples, you&rsquo;ve had to clip your vocabulary to 10,000 words and your embedding vector length is 300. With negative sampling we&rsquo;re updating a few positive words, and a few negative words (lets say <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex"> k = 10 </annotation></semantics></math></span>) which translates to only 3,000 individual weights in W<sup>&rsquo;</sup>. This represents 0.1% of the 3 million weight values we would otherwise be updating without negative sampling!</p><p>The probability that a negative word is chosen is determined using the following equation:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>I</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>w</mi><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msubsup><msub><mi>v</mi><msub><mi>w</mi><mi>I</mi></msub></msub><mo stretchy="false">)</mo><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi></mrow><mi>K</mi></munderover><msub><mi>E</mi><mrow><msub><mi>w</mi><mi>i</mi></msub><mtext> </mtext><msub><mi>P</mi><mi>n</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy="false">(</mo><mo>−</mo><msubsup><mi>v</mi><mi>w</mi><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msubsup><msub><mi>v</mi><msub><mi>w</mi><mi>I</mi></msub></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex"> \log p(w|w_I) = \log \sigma (v_w^{\prime T} v_{w_I}) + \sum_{i=k}^K E_{w_i ~ P_n(w)} [\log \sigma(-v_w^{\prime T} v_{w_I})]</annotation></semantics></math></span></div><p>As we can see, the primary difference between this and the standard stochastic gradient decent version is that now we include K observations. As for how large k should be, Mikolov et al. had this to say:</p><blockquote>Our experiments indicate that values of <i>k</i> in the range 5–20 are useful for small training datasets, while for large datasets the <i>k</i> can be as small as 2–5.<p style=text-align:right;padding-bottom:0;padding-top:1rem>- Mikolov et al. (<a href=#fn:fn1>2013</a>)</p></blockquote><p>Something to keep in mind however is that the official C implementation uses a slightly different formulation as seen below<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>:</p><div class=math-block><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mrow><mo fence="true">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>j</mi></msub><msup><mo stretchy="false">)</mo><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup><mo fence="true">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex"> P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^n \left( f(w_j)^{3/4} \right)} </annotation></semantics></math></span></div><p>Note that even without negative sampling, only the weights for the target word in matrix W are updated.</p><h2>Conclusion</h2><p>Thanks for reading, I hope you found this useful! If anything still seems unclear, please let me know so I can improve this tutorial. For further reading, I highly suggest working through each of the references below.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. <a class=inline_link target=_blank rel="noopener noreferrer" href=https://arxiv.org/abs/1301.3781><i>Efficient Estimation of Word Representations in Vector Space</i></a> (Submitted on 16 Jan 2013)&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. <a class=inline_link target=_blank rel="noopener noreferrer" href=https://arxiv.org/abs/1310.4546><i>Distributed Representations of Words and Phrases and their Compositionality</i></a> (Submitted on 16 Oct 2013)&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Chris McCormick. <a target=_blank rel="noopener noreferrer" href=http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/><i>Word2Vec Tutorial Part 2 - Negative Sampling</i></a> (January 11, 2017)&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></body></html>