<!doctype html><html lang=en><head><link rel=preload href=/fonts/figtree-v1-latin-regular.woff2 as=font type=font/woff2 crossorigin><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=author content="Nathan Rooy"><meta name=copyright content="Nathan A. Rooy"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=language content="en"><meta http-equiv=content-language content="en-gb"><link rel=icon type=image/png href=/favicon/favicon.png><script defer data-domain=nathanrooy.github.io src=https://plausible.io/js/plausible.js></script><script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta property="og:title" content="Nathan A. Rooy | nathanrooy.github.io"><meta property="og:description" content="Nathan A. Rooy | nathanrooy.github.io"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:url" content="https://nathanrooy.github.io/posts/2019-02-06/raspberry-pi-deep-learning-traffic-tracker/"><meta property="og:site_name" content="nathanrooy.github.io"><meta property="og:image" content="https://nathanrooy.github.io/about/DSC_3706-EDIT-BLOG-TINY.jpg"><title>Nathan Rooy</title><link rel=canonical href=https://nathanrooy.github.io/posts/2019-02-06/raspberry-pi-deep-learning-traffic-tracker/ itemprop=url><base href=https://nathanrooy.github.io/posts/2019-02-06/raspberry-pi-deep-learning-traffic-tracker/><meta itemprop=name content="Nathan A. Rooy | nathanrooy.github.io"><meta itemprop=description content="Nathan A. Rooy | nathanrooy.github.io"><meta itemprop=image content="https://nathanrooy.github.io/about/DSC_3706-EDIT-BLOG-TINY.jpg"><meta name=description content="Nathan A. Rooy | nathanrooy.github.io"><meta name=application-name content="nathanrooy.github.io"><meta name=twitter:card content="nathanrooy.github.io"><meta name=twitter:url content="https://nathanrooy.github.io/posts/2019-02-06/raspberry-pi-deep-learning-traffic-tracker/"><meta name=twitter:title content="Nathan A. Rooy | nathanrooy.github.io"><meta name=twitter:description content="Nathan A. Rooy | nathanrooy.github.io"><meta name=twitter:image content="https://nathanrooy.github.io/about/DSC_3706-EDIT-BLOG-TINY.jpg"><link rel=alternate type=application/rss+xml href=https://nathanrooy.github.io/index.xml title="Nathan Rooy"><style>/*!normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css*/html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:initial}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,[type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}template{display:none}[hidden]{display:none}:root{--main-text-color:#222;--main-text-weight:400;--text-bold:600;--font-family:"Figtree", sans-serif;--background-light:#F5F5F5}body{-webkit-font-smoothing:antialiased}@font-face{font-family:figtree;font-style:normal;font-weight:400;font-display:swap;src:url(/fonts/figtree-v1-latin-regular.eot);src:local(''),url(/fonts/figtree-v1-latin-regular.eot?#iefix)format('embedded-opentype'),url(/fonts/figtree-v1-latin-regular.woff2)format('woff2'),url(/fonts/figtree-v1-latin-regular.woff)format('woff'),url(/fonts/figtree-v1-latin-regular.ttf)format('truetype'),url(/fonts/figtree-v1-latin-regular.svg#Figtree)format('svg')}@font-face{font-family:figtree;font-style:normal;font-weight:600;font-display:swap;src:url(/fonts/figtree-v1-latin-600.eot);src:local(''),url(/fonts/figtree-v1-latin-600.eot?#iefix)format('embedded-opentype'),url(/fonts/figtree-v1-latin-600.woff2)format('woff2'),url(/fonts/figtree-v1-latin-600.woff)format('woff'),url(/fonts/figtree-v1-latin-600.ttf)format('truetype'),url(/fonts/figtree-v1-latin-600.svg#Figtree)format('svg')}.home-page{font-family:var(--font-family);color:var(--main-text-color);margin-top:0}.home-page ul li{margin-bottom:.3rem}.home-page ul a{color:var(--main-text-color)}.post-li-title{width:95%;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;float:left}.post-li-date{float:left;padding-bottom:1em;font-size:16px;color:#a9a9a9}@media only screen and (min-width:501px){.wrapper{max-width:660px;margin:0 auto;padding:1rem}.container{margin-top:6rem}.post-li-title{font-size:19px}.post-ul{list-style-type:none;padding-left:0;list-style:none;line-height:1.6em;margin-top:0}.post-li{overflow:hidden}}@media only screen and (max-width:950px){.container{margin-top:0}}@media only screen and (max-width:500px){.wrapper{margin:0 auto}.container{margin:2rem 1rem 1rem}.post-ul{list-style-type:none;padding-left:0;list-style:none;line-height:1.5em;margin-top:.2rem}.post-li{overflow:hidden}}.nyc-progress-image{margin-top:-1rem}.strava-grid{padding-top:1rem}.language-python{font-size:.8rem;line-height:0}.highlight{margin-bottom:1rem}.personal-social-media{text-align:center}.personal-social-media a{display:inline-block;padding-left:5px;padding-right:5px}.about h3{margin-top:0;padding-top:0}.about li{margin-bottom:0;padding-bottom:0}.about-ul li{margin-bottom:.5rem}footer{padding-bottom:6rem;font-family:var(--font-family);font-size:.75em;color:#999;margin-top:3rem;line-height:1.2rem}footer a{color:#555;font-weight:var(--main-text-weight)}footer a:hover{text-decoration:underline;color:#555}footer a:visited{color:#555}.footnotes{line-height:1rem;font-size:15px;margin-top:1em}.footnotes hr{display:none}.footnotes li p{line-height:1rem;margin-bottom:0}.footnotes ol{list-style:none;counter-reset:list;padding-left:2.5em;font-size:15px}.footnotes ol>li{list-style:none;position:relative}.footnotes ol>li:before{counter-increment:list;content:"[" counter(list)"] ";position:absolute;left:-2.5em;font-size:15px}ul.flat{margin:0;padding:0}ul.flat li{display:inline-block;list-style:none;margin-left:0}.markdown .post-update{background-color:var(--background-light);border-left:10px solid;border-color:#e74c3c;padding-top:1.5rem;padding-left:1.5rem;padding-bottom:1.5rem;padding-right:1.5rem;margin-top:1rem;margin-left:0;margin-right:0;margin-bottom:2rem;line-height:1.5rem;font-weight:var(--main-text-weight);font-size:15px}.highlighter-rouge{position:relative;font-size:.75rem;color:var(--main-text-color);border:1px solid;border-color:#e6e6e6;border-radius:3px;background-color:#f2f2f2;padding:.1rem .2rem}.figcaption{font-size:.8rem;padding-bottom:1rem}.markdown .photoset-grid-lightbox{margin-bottom:1rem}.markdown{font-family:var(--font-family);color:var(--main-text-color);padding-bottom:3rem}.markdown ol{font-weight:var(--main-text-weight);font-size:16px;margin-top:0;padding-bottom:0;line-height:1.2rem;padding-right:2em;width:85%;margin-block-end:.25em}.markdown ol li{padding-bottom:.75em}.markdown p{font-weight:var(--main-text-weight);font-size:16px;margin-top:0;margin-bottom:1rem;line-height:1.7rem;letter-spacing:.02rem}.markdown blockquote{background-color:#f2f2f2;border-left:10px solid;border-color:#2dad60;padding-top:1.5rem;padding-left:1.5rem;padding-bottom:1.5rem;padding-right:1.5rem;margin-top:0;margin-left:0;margin-right:0;margin-bottom:1rem;line-height:1.5rem;font-weight:var(--main-text-weight);font-size:15px}.markdown blockquote p{margin-bottom:0}.markdown h2{font-family:figtree;font-size:1.2rem;font-weight:var(--text-bold);margin-bottom:.4em}*+h2{padding-top:2rem}.markdown .ul-style-b li{margin-bottom:.5rem;font-size:16px}.TLDR{font-family:var(--font-family);padding-top:0;margin-top:0;padding-bottom:4rem;margin-bottom:0;font-size:1.1rem;line-height:1.7rem;font-weight:var(--main-text-weight);letter-spacing:.02rem}table{width:100%;font-size:14px;letter-spacing:-.01em;border-collapse:collapse;margin-bottom:2em;margin-top:2em;font-weight:var(--main-text-weight);line-height:1.2rem}table th{text-align:left}table thead{border-bottom:1px solid var(--main-text-color);line-height:1.4em}.post-header h1{font-weight:var(--main-text-weight);font-size:2.3rem;font-family:var(--font-family);color:var(--main-text-color);line-height:2.64rem}.post ul li{margin-bottom:10px}.post ul li p{display:inline}.post .post-header{margin-bottom:5rem}.post .post-header .title{margin:0}.post .post-header .meta{padding-left:0;margin-top:10px}.post .post-header a{color:#999}.post .post-header a:hover{text-decoration:underline}.post .post-header a:visited{color:#999}.separator{display:flex;align-items:center;padding-top:2rem;padding-bottom:1.5rem}.separator::before,.separator::after{content:'';flex:1;border-bottom:1px solid #000}.separator:not(:empty)::before{margin-right:.6rem}.separator:not(:empty)::after{margin-left:.6rem}.separator #nav-icons-footer svg{margin-bottom:-.05rem;display:inline-block;color:var(--main-text-color)}a{text-decoration:none;color:#0955b5}a:hover{text-decoration:underline}a:visited{color:#205caa}::selection{background:#fff9c4;text-shadow:none}img{margin:1.5em auto;max-width:100%;display:block}a img{border:none}figure{margin:0;text-align:center}ul{padding-left:15px}ul{list-style:disc inside}.markdown ul li{font-size:15px}.highlight pre{margin-bottom:0;margin-top:0;padding:20px;background-color:initial!important;overflow:auto;word-wrap:normal;white-space:pre}.highlight{background:0 0;background-color:#fafafa}.meta{font-family:var(--font-family);font-size:.8rem;color:#999;margin-bottom:4px}.list .posts .post .meta{margin-bottom:0;margin-left:5px}.markdown .photoset-grid-lightbox{margin-bottom:1rem}table .katex-display{margin-top:.2rem;margin-bottom:.2rem}.post ul li .katex-display{display:inline-block;list-style:none;margin-left:0;margin-bottom:0;line-height:0;padding:0}@media only screen and (min-width:501px){#burger{padding-left:2rem;padding-top:2rem}}@media only screen and (max-width:950px){#burger{padding-left:1rem;padding-top:1rem;padding-bottom:1.5rem}}@media only screen and (max-width:500px){#burger{padding-left:1rem;padding-top:1rem}}#burger{font-family:var(--font-family);font-weight:400;color:#a9a9a9;font-size:1.5rem}#burger a{display:inline-block;padding-right:.8rem}#burger label{cursor:pointer}#burger label:hover{color:var(--main-text-color)}#nav-icons{padding-left:.8rem}#nav-icons svg{max-height:20px;margin-bottom:-.05rem;color:var(--main-text-color)}#l1-val{position:relative;top:-.12rem}#l2-val{position:relative;top:-.2rem;padding-left:.1rem;padding-right:.1rem}#burger-cbox{display:none}#label-1{display:inline-block}#label-2{display:none}#burger-cbox:not(:checked)~#nav-icons{display:none}#burger-cbox:checked~#nav-icons{display:inline-block}#burger-cbox:checked~#label-1{display:none}#burger-cbox:checked~#label-2{display:inline-block;color:var(--main-text-color)}</style></head><body><div class=header><nav role=navigation><div id=burger><input type=checkbox id=burger-cbox autocomplete=off>
<label id=label-1 for=burger-cbox><span>[</span><span id=l1-val>+</span><span>]</span></label>
<label id=label-2 for=burger-cbox><span>[</span><span id=l2-val>x</span><span>]</span></label><div id=nav-icons><a href=/><svg xmlns="http://www.w3.org/2000/svg" id="home-icon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg></a><a href=/reading><svg width="24" height="24" viewBox="0 0 23.809855 18.892912" id="svg5" inkscape:version="1.1 (c4e8f9e, 2021-05-24)" sodipodi:docname="book.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" fit-margin-top="0" fit-margin-left="0" fit-margin-right="0" fit-margin-bottom="0" inkscape:zoom=".69130191" inkscape:cx="77.39021" inkscape:cy="446.98271" inkscape:window-width="1920" inkscape:window-height="1011" inkscape:window-x="0" inkscape:window-y="0" inkscape:window-maximized="1" inkscape:current-layer="g39"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-6.352941,-30.26865)"><g id="g39" transform="matrix(0.26458333,0,0,0.26458333,5.0300244,26.48677)"><path class="st0" d="m92.22 22.9v59C74.24 76.35 61.26 78.85 54.73 81.04 59.2 76.64 68.54 70.55 84.16 74.14l1.69.39V21.42c2.17.29 4.3.77 6.37 1.48z" id="path11"/><path class="st0" d="M45.24 81.04C38.73 78.85 25.74 76.35 7.77 81.9v-59c2.06-.71 4.2-1.19 6.37-1.48v53.11l1.68-.39c15.61-3.58 24.96 2.5 29.42 6.9z" id="path13"/><polygon points="48.86,85.63 48.82,85.58 48.91,85.63" id="polygon15"/><polygon points="51.17,85.58 51.13,85.63 50,85.63 51.09,85.62" id="polygon17"/><path d="m94.09 20.63c-2.68-1-5.45-1.65-8.24-2.02v-2.94l-1.08-.24c-17.8-4.09-28.32 3.81-32.9 8.68-.81.87-1.44 1.63-1.88 2.22C49.54 25.74 48.91 24.98 48.1 24.11c-4.58-4.87-15.11-12.76-32.9-8.68l-1.07.24v2.94c-2.8.38-5.57 1.02-8.24 2.02L5 20.97V85.7l1.81-.61c13.49-4.49 24.13-4.16 30.68-3.08 7.09 1.15 11.05 3.4 11.09 3.42l.23.14.09.06H50l1.09-.01.08-.05.23-.14c.05-.02 4-2.27 11.09-3.42 6.55-1.08 17.19-1.41 30.68 3.08l1.82.61V20.97zM7.78 81.89v-59c2.06-.71 4.2-1.19 6.37-1.48v53.11l1.68-.39c15.61-3.58 24.95 2.5 29.42 6.9-6.52-2.18-19.5-4.68-37.47.86zm40.76-1.3c-4.19-4.66-14.29-12.83-31.63-9.5v-53.2c11.11-2.28 18.93.38 24.02 3.66 2.6 1.68 4.48 3.52 5.71 4.95 1.14 1.32 1.73 2.28 1.82 2.43l.08.15zm2.76.17V29.34l.24-.4c.09-.16.68-1.12 1.82-2.44 1.23-1.43 3.11-3.27 5.71-4.94 5.08-3.29 12.91-5.95 24.02-3.67v53.2C65.54 67.72 55.4 76.13 51.3 80.76zm40.92 1.13C74.24 76.34 61.26 78.84 54.73 81.03 59.2 76.63 68.54 70.54 84.16 74.13l1.69.39v-53.1c2.16.29 4.3.77 6.37 1.48z" id="path19"/></g></g><style id="style9">.st0{fill:none}</style></svg></a><a href=/nyc_biking><svg viewBox="0 0 91.5 52.5" width="32" height="24" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"><g transform="matrix(-1,0,0,1,95.8,-23.9)"><path d="m78 40.6c-3.8.0-7.3 1.2-10.2 3.2l-7.4-9.4 2.9-8.5H67c.6.0 1-.4 1-1s-.4-1-1-1H54.8c-.6.0-1 .4-1 1s.4 1 1 1h6.3l-2.3 6.7c-.2-.2-.4-.3-.7-.3H32l2.2-6.4h5.7c.6.0 1-.4 1-1s-.4-1-1-1h-6.4c-.4.0-.8.3-.9.7l-5.7 16.8c-1.5-.4-3.1-.7-4.8-.7-9.8.0-17.8 8-17.8 17.8s8 17.8 17.8 17.8 17.8-8 17.8-17.8c0-7.5-4.6-13.9-11.2-16.5l2.6-7.6 19 24.4c.1.3.3.6.6.7.1.0.2.1.3.1h.1 8.9c.5 9.4 8.3 16.8 17.8 16.8 9.8.0 17.8-8 17.8-17.8s-8-18-17.8-18zM37.8 58.4c0 8.7-7.1 15.8-15.8 15.8S6.2 67.1 6.2 58.4 13.3 42.6 22 42.6c1.4.0 2.8.2 4.1.6L21 58.1c-.2.5.1 1.1.6 1.3.1.0.2.1.3.1.4.0.8-.3.9-.7l5.1-14.9c5.9 2.2 9.9 7.9 9.9 14.5zm29.7-11.8L76 57.4H62.2c.3-4.3 2.2-8.1 5.3-10.8zm-16.7 9.6-17.1-22h24l.4.5zm1.8 1.2 7.1-20.8 6.6 8.4c-3.5 3.1-5.7 7.4-6 12.4zM78 74.2c-8.4.0-15.2-6.6-15.8-14.8H78c.4.0.7-.2.9-.6.2-.3.1-.8-.1-1.1L69 45.4c2.5-1.8 5.6-2.8 8.9-2.8 8.7.0 15.8 7.1 15.8 15.8s-7 15.8-15.7 15.8z"/></g></svg></a><a href=/about><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user" id="about-icon"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg></a><a href=/index.xml><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 30 24" fill="none" stroke="#444" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss" id="rss-icon"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></div></div></nav></div><div class="container wrapper post"><div class=post-header><h1 class=title>A Low Cost Approach to Improving Pedestrian Safety with Deep Learning</h1><div class=meta><a href=/>Nathan Rooy</a> | April 17, 2018</div></div><p class=TLDR><strong>TL;DR</strong> - Using TensorFlow and a Raspberry Pi, I developed a cheap and accurate way of counting both pedestrians and vehicle traffic.</p><div class=markdown><style>.plot_style line{stroke:grey}.plot_style path{stroke:grey}.plot_style text{fill:grey;font-size:1em;font-family:open sans;font-style:normal;font-weight:300}</style><p class=post-update><b>POST UPDATE</b>: Since posting this, I've received a considerable amount of interest from individuals and local city governments from across the U.S. looking to get a better grasp on pedestrian data. Because of this, I spent some time building out a better algorithm and packaging it for easy use. I'm terrible with names, so for now it's simply "urban mobility tracker". If you can think of a better name, please let me know! Additionally, to generate detections for the urban-mobility-tracker, I performed a full model retraining of MobileNetV1 SSD using data I collected and annotated. I'm calling this new pedestrian/vehicle/bicyclist optimized object detector "PedNet". As time goes on and more people use these two tools, I'm hoping that people will contribute more training data (especially in adverse lighting and weather conditions) so that PedNet can be retrained and improved. Lastly, this new version continues to use TensorFlow but with the addition of the <a target=_blank href=https://coral.ai/products/accelerator/>Coral USB accelerator</a> which leverages Google's TPU technology so now it achieves an update frequency of ~10Hz!!! (old version was ~2Hz)<br><br>GitHub links:<br>• Urban-Mobility-Tracker [<a target=_blank href=https://github.com/nathanrooy/rpi-urban-mobility-tracker>here</a>]<br>• PedNet [<a target=_blank href=https://github.com/nathanrooy/ped-net>here</a>]</p><h2>Introduction</h2><p>As someone who <a target=_blank href=../../2018-01-11/one-year-of-bike-commuting/>bikes to work everyday</a>, I&rsquo;m acutely aware of how important good urban design and planning is and its impact on pedestrian safety as well as the overall well-being of a neighborhood. I&rsquo;m also aware of how difficult it is to convince car dependent, suburban commuters that we need to invest our money into bike lanes and <a target=_blank href=https://en.wikipedia.org/wiki/Curb_extension>curb extensions</a> rather than lane extensions and parking lots. Because of this, I&rsquo;m always a bit bummed when I see unfounded internet shouting matches over why the city should/should not implement bike and pedestrian safety measures. We should let the data decide what happens, not ideology or personal bias. Unfortunately, it&rsquo;s this data that&rsquo;s hard to come by.</p><p>I remember when shortly after the <a target=_blank href=https://www.cincinnati-oh.gov/bikes/news/central-parkway-bikeway/>bike lanes on Central Pkwy</a> were installed, they were looking for volunteers to stand out in the baking sun all day and manually count cyclists as they rode by. This struck me as comically old-fashioned but also a bit sad because this was apparently the only way to get the data at the time. Fast forward almost five years and the situation appears to not have improved much. Below is a picture I took recently of some pneumatic tubes used to count traffic.</p><img alt="pneumatic tube traffic counter" title="pneumatic tube traffic counter" src=pneumatic-tubes.jpg>
<center class=figcaption>Figure 1: Pneumatic tube based traffic counting</center><p>These are great for temporarily counting cars and other large vehicles. Counting bikes, and pedestrians, not so much&mldr; When I rode over them that morning on my bike, I was indistinguishable from a motorcycle. I&rsquo;m an invisible cyclist, so why should the city prioritize this road for a bike lane? Additionally, this setup was in place for less than a day because those tubes are held in place using nails pounded into the road surface and reinforced with a couple strips of Kentucky chrome. Definitely not ideal.</p><center><table style=width:400px><thead><tr><th style=text-align:left;padding-left:1em><strong>System</strong></th><th style=text-align:center><strong>Type</strong></th><th style=text-align:center><strong>Cost</strong></th></tr></thead><tbody><tr><td style=text-align:left;padding-left:1em>SenSource</td><td style=text-align:center>pneumatic</td><td style=text-align:center>$695</td></tr><tr><td style=text-align:left;padding-left:1em>RoadPod</td><td style=text-align:center>pneumatic</td><td style=text-align:center>$1,150</td></tr><tr><td style=text-align:left;padding-left:1em>JAMAR TRAX I Plus</td><td style=text-align:center>pneumatic</td><td style=text-align:center>$2,600</td></tr><tr><td style=text-align:left;padding-left:1em>Wavetronix SmartSensor HD</td><td style=text-align:center>radar</td><td style=text-align:center>$6,700</td></tr><tr><td style=text-align:left;padding-left:1em>Miovision Technologies VCU</td><td style=text-align:center>video</td><td style=text-align:center>$3,700</td></tr></tbody></table></center><center class=figcaption>Table 1: Selection of traffic counting systems</center><p>Although other counting systems exist, they tend to be overly expensive, invasive, and unable to simultaneously count both pedestrians and vehicle traffic.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> Cincinnati is full of bikers, I see them every day. But if they&rsquo;re not being counted, we&rsquo;ll never be able to overcome the suburban bias in Cincinnati&rsquo;s urban design and planning. With a better counting system in place we can get the necessary data to make informed decisions that reflect reality, resulting in a fairer and more equitable city. If only there was a low-cost, non-invasive solution that could passively count all types of traffic simultaneously in any weather condition 24/7/365 and seamlessly push that information to the cloud on a minute by minute basis&mldr;</p><h2>My Solution</h2><p>After thinking about this for a while, I decided that I could probably do a better job than what currently exists. Given that low-cost and high accuracy are my two primary goals, I went with a <a target=_blank href=https://en.wikipedia.org/wiki/Raspberry_Pi>Raspberry Pi Zero</a> which is the smallest/cheapest of the Raspberry Pi models with the 8-megapixel v2 NoIR (infrared) camera and a rechargeable usb battery pack. Since my system is vision based it needs a clear line of sight to the street but also somehow remain hidden from people who might tamper with it. Given these two contradictory constraints of being hidden yet visible, I went with a stealth approach. This was accomplished by buying a plastic weatherproof electrical box and cutting a hole on the front for the camera. The camera hole was sealed with a clear lens filter. Lastly, I mounted six stupidly strong <a target=_blank href=https://en.wikipedia.org/wiki/Neodymium_magnet>neodymium magnets</a> on the back so I could attach it to street lights, sign posts, etc.</p><img alt="deep learning raspberry pi traffic tracker" title="deep learning raspberry pi traffic tracker" src=traffic-tracker-close-up.jpg>
<center class=figcaption>Figure 2: My deep learning, raspberry pi based traffic tracker</center><p>To actually count pedestrians and vehicle traffic I built out a <a target=_blank href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> (CNN) with a secondary region proposal network (R-CNN) using <a target=_blank href=https://www.tensorflow.org/>TensorFlow</a> and <a target=_blank href=https://www.python.org/>Python</a>. This allows for both the detection and localization of objects within the frame. Actual object tracking was accomplished using a light weight temporal clustering scheme using <a target=_blank href=http://www.numpy.org/>NumPy</a> and <a target=_blank href=http://scikit-learn.org>scikit-learn</a>. For a more in-depth look, click [<a href=#technical-notes>here</a>] to read the technical notes section.</p><img alt="deep learning raspberry pi traffic tracker in the wild" title="deep learning raspberry pi traffic tracker in the wild" src=in-the-field-03.jpg>
<center class=figcaption>Figure 3: Hidden in plain sight - my baby out in the wild!</center><p>After a week or so of casual coding I had a finished system. Given that the tracking algorithm is built on top of open-source software, the only costs associated with this system arise from the hardware side.</p><center><table style=width:250px><thead><tr><th style=text-align:left;padding-left:1em><strong>Item</strong></th><th style=text-align:center;padding-right:.5em><strong>Cost</strong></th></tr></thead><tbody><tr><td style=text-align:left;padding-left:1em>Raspberry Pi Zero</td><td style=text-align:center;padding-right:.5em>$10</td></tr><tr><td style=text-align:left;padding-left:1em>Plastic electrical box</td><td style=text-align:center;padding-right:.5em>$14</td></tr><tr><td style=text-align:left;padding-left:1em>Magnets</td><td style=text-align:center;padding-right:.5em>$18</td></tr><tr><td style=text-align:left;padding-left:1em>128GB usb storage</td><td style=text-align:center;padding-right:.5em>$28</td></tr><tr><td style=text-align:left;padding-left:1em>USB Battery pack</td><td style=text-align:center;padding-right:.5em>$40</td></tr><tr><td style=text-align:right><strong>Total</strong></td><td style=text-align:center;color:red;padding-right:.5em><strong>$110</strong></td></tr></tbody></table></center><center class=figcaption>Table 2: Cost breakdown of my Raspberry Pi based system</center><p>As an example of what all this looks like in action, below is a gif showing the tracking algorithm when it was placed at random spots throughout Cincinnati.</p><img alt="deep learning raspberry pi traffic tracker in the wild" title="deep learning raspberry pi traffic tracker in the wild" src=traffic-tracker.gif>
<center class=figcaption>Figure 4: Tracking traffic</center><h2>Results</h2><p>To test my system out, I placed it at few areas throughout the city. I started out by leaving it out in the field for 24 hours at a time. After I got comfortable with its performance I extended that duration to a full week (usually Sunday night to Sunday night).</p><img src=central-pkwy-location-90.jpg>
<center class=figcaption>Figure 5: Central Pkwy location</center><p>As an example of the data that this system produces, below are the results for when it was placed on the Colerain Ave. bridge right as it passes over Central Pkwy during August 23, 2018.</p><div id=plot_3></div><center class=figcaption>Figure 6: Vehicle counts on Central Pkwy</center><p>Because the system generates a time stamp for each object as it passes over an imaginary “start/finish”, I binned all these data points into five-minute blocks. Additionally, even though vehicle traffic can be further broken down by vehicle type, it was easier to just group all vehicles into a single class. Above in figure 6, we can see the influx of southbound commuters coming into the city during the morning rush as well as when they leave in the evening rush headed north. Vehicle traffic peaked at 88 northbound vehicles per five minutes around 5:20pm. The day ended up with a total of 6982 southbound and 7092 northbound vehicles travelling on this particular segment of Central Pkwy.</p><div id=plot_1></div><center class=figcaption>Figure 7: Bicycle counts on Central Pkwy</center><p>Because the frequency of bicycle and pedestrian traffic was so much smaller relative to vehicle traffic, it was easier to display this data from a cumulative perspective. Above in figure 7, the bicycle count on Central Pkwy can be seen. Even with the cumulative form, we can see a large amount of growth during the southbound morning rush into the city, but compared to vehicle traffic, the evening bicycle rush is more diffused. Total bicycle traffic for the day capped out at 56 and 57 for the northbound and southbound directions respectively.</p><div id=plot_2></div><center class=figcaption>Figure 8: Pedestrian counts on Central Pkwy</center><p>Lastly, the pedestrian traffic can be seen above in figure 8. Unlike both vehicle and bicycle traffic, pedestrian traffic on Central Pkwy doesn’t have large spikes during the morning / evening rushes. Instead, pedestrian traffic is diffused fairly evenly throughout the entire day.</p><script src=https://d3js.org/d3.v4.min.js></script><script>var margin={top:50,right:20,bottom:30,left:25},width=650-margin.left-margin.right,height=300-margin.top-margin.bottom,parseTime=d3.timeParse("%Y-%m-%d %H:%M:%S"),x=d3.scaleTime().range([0,width]),y=d3.scaleLinear().range([height,0]),svg_1=d3.select("#plot_1").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")"),valueline1=d3.line().curve(d3.curveStepBefore).x(function(a){return x(a.TIME)}).y(function(a){return y(a.BIKE_S_CUM)}),valueline2=d3.line().curve(d3.curveStepBefore).x(function(a){return x(a.TIME)}).y(function(a){return y(a.BIKE_N_CUM)}),svg_2,valueline3,valueline4,svg_3,valueline5,valueline6;d3.csv("counts_b.csv",function(b,a){if(b)throw b;a.forEach(function(a){a.TIME=parseTime(a.TIME),a.BIKE_N_CUM=+a.BIKE_N_CUM,a.BIKE_S_CUM=+a.BIKE_S_CUM}),x.domain(d3.extent(a,function(a){return a.TIME})),y.domain([0,60]),svg_1.append("path").data([a]).attr("class","line").attr("d",valueline1).style('fill','none').style('stroke',d3.rgb(3,120,177)),svg_1.append("path").data([a]).attr("class","line").attr("d",valueline2).style('fill','none').style('stroke',d3.rgb(247,127,15)),svg_1.append("g").attr("class","plot_style").attr("transform","translate(0,"+height+")").call(d3.axisBottom(x)).selectAll("text").attr("y",'1em').attr("x",'-0.25em').style("text-anchor","start"),svg_1.append("g").attr("class","plot_style").call(d3.axisLeft(y)),svg_1.append("text").attr("transform","rotate(-90)").attr("y",5).attr("x",0).attr("dy","1em").style("text-anchor","end").attr('font-family','Open Sans').style("font-size","0.8em").style("fill","grey").text("Total number of bicycles"),svg_1.append("text").attr("x",width/2).attr("y",-(margin.top/2)).attr("text-anchor","middle").style('font-family','Open Sans').style("font-size","1em").style("fill","grey").text("Cumulative Bicycle Counts"),svg_1.append("rect").attr("x",width-100).attr("y",height-50-9-16).attr("width",18).attr("height",9).style('opacity',.8).style('fill',d3.rgb(247,127,15)),svg_1.append("text").attr("x",width-78).attr("y",height-50-16).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Northbound"),svg_1.append("rect").attr("x",width-100).attr("y",height-50-9).attr("width",18).attr("height",9).style('opacity',.9).style('fill',d3.rgb(3,120,177)),svg_1.append("text").attr("x",width-78).attr("y",height-50).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Southbound")}),svg_2=d3.select("#plot_2").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")"),valueline3=d3.line().curve(d3.curveStepBefore).x(function(a){return x(a.TIME)}).y(function(a){return y(a.PED_S_CUM)}),valueline4=d3.line().curve(d3.curveStepBefore).x(function(a){return x(a.TIME)}).y(function(a){return y(a.PED_N_CUM)}),d3.csv("counts_p.csv",function(b,a){if(b)throw b;a.forEach(function(a){a.TIME=parseTime(a.TIME),a.PED_N_CUM=+a.PED_N_CUM,a.PED_S_CUM=+a.PED_S_CUM}),x.domain(d3.extent(a,function(a){return a.TIME})),y.domain([0,65]),svg_2.append("path").data([a]).attr("class","line").attr("d",valueline3).style('fill','none').style('stroke',d3.rgb(3,120,177)),svg_2.append("path").data([a]).attr("class","line").attr("d",valueline4).style('fill','none').style('stroke',d3.rgb(247,127,15)),svg_2.append("g").attr("class","plot_style").attr("transform","translate(0,"+height+")").call(d3.axisBottom(x)).selectAll("text").attr("y",'1em').attr("x",'-0.25em').style("text-anchor","start"),svg_2.append("g").attr("class","plot_style").call(d3.axisLeft(y)),svg_2.append("text").attr("transform","rotate(-90)").attr("y",5).attr("x",0).attr("dy","1em").style("text-anchor","end").attr('font-family','Open Sans').style("font-size","0.8em").style("fill","grey").text("Total number of pedestrians"),svg_2.append("text").attr("x",width/2).attr("y",-(margin.top/2)).attr("text-anchor","middle").style('font-family','Open Sans').style("font-size","1em").style("fill","grey").text("Cumulative Pedestrian Counts"),svg_2.append("rect").attr("x",width-100).attr("y",height-50-9-16).attr("width",18).attr("height",9).style('opacity',.8).style('fill',d3.rgb(247,127,15)),svg_2.append("text").attr("x",width-78).attr("y",height-50-16).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Northbound"),svg_2.append("rect").attr("x",width-100).attr("y",height-50-9).attr("width",18).attr("height",9).style('opacity',.9).style('fill',d3.rgb(3,120,177)),svg_2.append("text").attr("x",width-78).attr("y",height-50).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Southbound")}),svg_3=d3.select("#plot_3").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")"),valueline5=d3.line().x(function(a){return x(a.TIME)}).y(function(a){return y(a.VEH_S)}),valueline6=d3.line().x(function(a){return x(a.TIME)}).y(function(a){return y(a.VEH_N)}),d3.csv("counts_v.csv",function(b,a){if(b)throw b;a.forEach(function(a){a.TIME=parseTime(a.TIME),a.VEH_S=+a.VEH_S,a.VEH_N=+a.VEH_N}),x.domain(d3.extent(a,function(a){return a.TIME})),y.domain([0,100]),svg_3.append("path").data([a]).attr("class","line").attr("d",valueline5).style('fill','none').style('stroke',d3.rgb(3,120,177)),svg_3.append("path").data([a]).attr("class","line").attr("d",valueline6).style('fill','none').style('stroke',d3.rgb(247,127,15)),svg_3.append("g").attr("class","plot_style").attr("transform","translate(0,"+height+")").call(d3.axisBottom(x)).selectAll("text").attr("y",'1em').attr("x",'-0.25em').style("text-anchor","start"),svg_3.append("g").attr("class","plot_style").call(d3.axisLeft(y)),svg_3.append("text").attr("transform","rotate(-90)").attr("y",5).attr("x",0).attr("dy","1em").style("text-anchor","end").attr('font-family','Open Sans').style("font-size","0.8em").style("fill","grey").text("Vehicles per five minutes"),svg_3.append("text").attr("x",width/2).attr("y",-(margin.top/2)).attr("text-anchor","middle").style('font-family','Open Sans').style("font-size","1em").style("fill","grey").text("Vehicles per Five Minutes"),svg_3.append("rect").attr("x",width-100).attr("y",height-50-9-16-100).attr("width",18).attr("height",9).style('opacity',.8).style('fill',d3.rgb(247,127,15)),svg_3.append("text").attr("x",width-78).attr("y",height-50-16-100).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Northbound"),svg_3.append("rect").attr("x",width-100).attr("y",height-50-9-100).attr("width",18).attr("height",9).style('opacity',.9).style('fill',d3.rgb(3,120,177)),svg_3.append("text").attr("x",width-78).attr("y",height-50-100).attr('font-family','Open Sans').style("font-size","0.8em").style("text-anchor","start").style('fill','grey').text("Southbound")})</script><h2 id=learnings>Learnings and Future Improvements</h2><p>Over the span of this project I was able to fully test the limits of this first prototype and came away with a few key points.</p><p><strong id=algo-improvements>Algorithm Improvements</strong> — Although the current two-part solution works, it is far from elegant and definitely not the most efficient solution. I am actively building out several new deep learning based solutions that combine the detection, localization, and tracking into a single operation within <a target=_blank href=https://www.tensorflow.org/mobile/tflite/>TensorFlow Lite</a> using a MobileNets<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> style framework. My hope is that these new versions will improve performance during heavy occlusion or low light while reducing computational overhead. In theory, if this were ever implemented with permission from a city (and thus solar-powered), I could potentially use something more energy intensive such as an NVIDIA Jetson or similar IoT board. The combination of the increased horsepower, and reduced computational load would hopefully allow for a frame rate closer to 10Hz rather than the current 2Hz.</p><p><strong>Energy Storage and Connectivity</strong> — Because I was flying under the radar during this project, I was limited in two areas; energy consumption and connectivity. Ideally I would have paired the 30,000mAh battery pack with a solar panel mounted on top. The solar panel would then power the Raspberry Pi and charge the battery pack during the day, while the battery pack alone would power it during the night and through overcast days.</p><p>The issue of connectivity would become the next obstacle in long-term deployment. As it currently stands, I use a 128GB usb memory card to store all the trajectory data. I could have paid for a Raspberry Pi cellular data plan, but I completely expected my prototype to get stolen. Given the ideal situation of being able to connect to an ethernet line, indefinite deployment becomes an actual possibility.</p><p><strong>Camera Placement</strong> — After a little trial and error, I discovered that the best position is ideally high over the center of the street with at least 100 yards of visibility. The hundred yards requirement was function of the slow frame rate of 2Hz. Presumably if I had faster hardware, I could increase the frame rate and reduce the required field of view. The elevation requirement is important because it reduces the occlusion, especially when there are multiple lanes of traffic. Lastly, I always tried to position the camera in a loosely northward facing direction and in close proximity to a street light. This allowed me to count traffic at night but also reduce the chances of incurring any glare or direct sun light which could wash out the images during the day.</p><p><strong>Thermal Management</strong> — The current version relies on passive cooling because weather sealing had the highest priority. Because of this limitation, I conservatively placed it in areas that were always shaded. This prevented me from measuring several road segments I was interested in but also forced me to get creative for others. If this were ever implemented with permission from a city, I would definitely build in a solar-powered fan for reducing mid-day heat buildup.</p><h2>Conclusion</h2><p>Overall I was really happy with the final results and I learned a lot about deploying a self-contained system into the wild. It would be nice to let this system harvest data for an extended period of time so that long-term trends could be accurately measured. With a system like this, a measurable impact of bike infrastructure investment could be generated. It could also provide the underlying data needed to justify new infrastructure proposals.</p><img alt="deep learning raspberry pi traffic tracker in the wild" title="deep learning raspberry pi traffic tracker in the wild" src=VineSt-2018-07-25-17-34-59-25.jpg>
<center class=figcaption>Figure 9: Me taking a picture of a Raspberry Pi taking a picture of me</center><p>Cincinnati government/department of transportation get at me. I want to help!</p><h2 id=technical-notes>Technical Notes</h2><p>Within this section, I&rsquo;ll cover the technical details in-depth. There are only a couple key components to my system, mainly the algorithm, training data, and hardware.</p><p><strong>Data</strong> — Using <a target=_blank href=https://github.com/tzutalin/labelImg>labelImg</a><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, I manually annotated examples of each class. This was fairly time-consuming, but it gave me good chuck of time to catchup on podcasts so it really wasn&rsquo;t too bad. I could have been substituted this manual data annotation with existing datasets such as the <a target=_blank href=http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/>Caltech Pedestrian Detection Benchmark</a> or one of the <a terget=_blank href=http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html>Daimler Pedestrian Benchmark Data Sets</a> but one of my primary goals with this project was to start at zero and finish with a solid working solution. I was also concerned with the discrepancy of training on full color data but operating with the infrared data coming from the v2 NoIR image sensor. Lastly, I was also curious as to whether my dataset could help overcome some known shortcomings in regards to Faster R-CNN.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><sup>,</sup><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> Below are the results from my manual annotation. Note that images are captured and analyzed at 1280x720 resolution.</p><center><table style=width:250px><thead><tr><th style=text-align:left;padding-left:1em><strong>Label</strong></th><th style=text-align:center;padding-right:.5em><strong>Annotations (#)</strong></th></tr></thead><tbody><tr><td style=text-align:left;padding-left:1em>Pedestrians</td><td style=text-align:center;padding-right:.5em>59</td></tr><tr><td style=text-align:left;padding-left:1em>Bicycles</td><td style=text-align:center;padding-right:.5em>259</td></tr><tr><td style=text-align:left;padding-left:1em>Cars</td><td style=text-align:center;padding-right:.5em>593</td></tr><tr><td style=text-align:left;padding-left:1em>SUVs</td><td style=text-align:center;padding-right:.5em>217</td></tr><tr><td style=text-align:left;padding-left:1em>Pickup Trucks</td><td style=text-align:center;padding-right:.5em>137</td></tr><tr><td style=text-align:left;padding-left:1em>Trucks</td><td style=text-align:center;padding-right:.5em>229</td></tr><tr><td style=text-align:left;padding-left:1em><strong>All annotations</strong></td><td style=text-align:center;padding-right:.5em><strong>1494</strong></td></tr></tbody></table></center><center class=figcaption>Table 3: Manual annotations (based off 529 images)</center><p><strong>Machine Learning</strong> — There are two primary parts to the tracking algorithm. The first is the <a target=_blank href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> based object detection and localization. For this I utilized a TensorFlow based implementation of Faster R-CNN<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, which is the new and improved version of Fast R-CNN<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>, which is the new and improved version of R-CNN<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. This particular implementation of Faster R-CNN was built on top of Res-Net101<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> which had been trained on the COCO<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> data set. I could have went with a faster single shot detection (SSD) framework such as YOLO/YOLOv3<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>, but I was more concerned with positional accuracy<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> because at the time my secondary goal was to measure vehicle/pedestrian speed. Training was carried out using the GeForce GTX 1050 GPU on my laptop which ended up taking roughly three hours.</p><p>The second component is the tracking aspect of the algorithm. Once the objects in the current time frame have been detected and localized, they are clustered using <a target=_blank href=means_clustering>k-means</a> over a time window of three frames. At an image capture rate of 2Hz, this translates to one second. This worked decently well for this initial prototype, but definitely has its limitations. I touch on several ways in which this can be improved [<a href=#algo-improvements>here</a>]. Below I made a general flow chart depicting the tracking process.</p><img alt="temporal clustering" title="temporal clustering" src=clustering.jpg>
<center class=figcaption>Figure 10: Tracking system</center><p><strong>Hardware</strong> — This initial version uses the v2 Pi NoIR 8-megapixel/1080p30 (Sony IMX219 sensor) camera paired with the Raspberry Pi Zero. The lack of computing power resulted in the tracking algorithm unable to exceed a frame rate of 2Hz. All of this was powered by a 30,000mAh USB battery pack. In hindsight, I should have probably just gotten the Raspberry Pi 3 B+ with some kind of small industrial battery pack.</p><h2>References</h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a target=_blank href=https://miovision.com/wp-content/uploads/Accuracy_Comparison_of_Non_Intrusive_Automated_Traffic_Volume_Counting_Equipment_Alb-1.pdf><i>Accuracy Comparison of Non‐Intrusive, Automated Traffic Volume Counting Equipment</i></a>. (October 2009) <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, H. Adam, <a target=_blank href=https://arxiv.org/abs/1704.04861><i>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</i></a>. (Submitted on 17 April 2017) <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>TzuTa Lin, <i>LabelImg</i>, <a target=_blank href=https://github.com/tzutalin/labelImg><a href=https://github.com/tzutalin/labelImg>https://github.com/tzutalin/labelImg</a></a>. (2015) <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>L. Zhang, L. Lin, X. Liang, K. He, <a target=_blankl href=https://arxiv.org/abs/1607.07032><i>Is Faster R-CNN Doing Well for Pedestrian Detection?</i></a>. (Submitted on 24 July 2016) <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>C. Eggert, S. Brehm, A. Winschel, D. Zecha and R. Lienhart, <a target=_blank href=https://ieeexplore.ieee.org/document/8019550/><i>A Closer Look: Small Object Detection in Faster R-CNN</i></a>. 2017 IEEE International Conference on Multimedia and Expo (ICME), Hong Kong, 2017, pp. 421-426. <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>S. Ren, K. He, R. Girshick, J. Sun, <a target=_blank href=https://arxiv.org/abs/1506.01497><i>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</i></a>. (Submitted on 4 June 2015) -> GitHub [<a target=_blank href=https://github.com/ShaoqingRen/faster_rcnn>here</a>] <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>R. Girshick, <a target=_blank href=https://arxiv.org/abs/1504.08083><i>Fast R-CNN</i></a>. (Submitted on 30 April 2015) -> GitHub [<a target=_blank href=https://github.com/rbgirshick/fast-rcnn>here</a>] <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>R. Girshick, J. Donahue, T. Darrell, J. Malik, <a target=_blank href=https://arxiv.org/abs/1311.2524><i>Rich feature hierarchies for accurate object detection and semantic segmentation</i></a>. (Submitted on 11 November 2013) -> GitHub [<a target=_blank href=https://github.com/rbgirshick/rcnn>here</a>] <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>K. He, X. Zhang, S. Ren, J. Sun, <a target=_blank href=https://arxiv.org/abs/1512.03385><i>Deep Residual Learning for Image Recognition</i></a>. Submitted on 10 December 2015) -> GitHub <a target=_blank href=https://github.com/KaimingHe/deep-residual-networks>here</a>] <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, P. Dollár, <a target=_blank href=https://arxiv.org/abs/1405.0312><i>Microsoft COCO: Common Objects in Context</i></a>. (Submitted on 1 May 2014) -> Data link [<a target=_blank href=http://cocodataset.org>here</a>] <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11 role=doc-endnote><p>J. Redmon, A. Farhadi, <a target=_blank href=https://arxiv.org/abs/1804.02767><i>YOLOv3: An Incremental Improvement</i></a>. (Submitted on 8 April 2018) <a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12 role=doc-endnote><p>J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, K. Murphy, <a target=_blank href=https://arxiv.org/abs/1611.10012><i>Speed/accuracy trade-offs for modern convolutional object detectors</i></a>. (Submitted on 30 November 2016) <a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div class=separator><div id=nav-icons-footer><a onclick="plausible('home-icon',{props:{method:'A Low Cost Approach to Improving Pedestrian Safety with Deep Learning'}})" href=/ alt="home page icon"><svg viewbox="0 0 24 24" height="20" style="padding-bottom:.15rem"><use href="#home-icon"/></svg></a><a onclick="plausible('about-icon',{props:{method:'A Low Cost Approach to Improving Pedestrian Safety with Deep Learning'}})" href=/about alt="about page icon"><svg viewbox="-1 2 24 24" height="20" style="padding-bottom:.05rem;padding-left:.5rem;padding-right:.5rem"><use href="#about-icon"/></svg></a><a onclick="plausible('rss-icon',{props:{method:'A Low Cost Approach to Improving Pedestrian Safety with Deep Learning'}})" href=/index.xml alt="rss page icon"><svg viewbox="0 0 20 24" height="25"><use href="#rss-icon"/></svg></a></div></div></div><footer><center>&copy; <a href=/about>Nathan A. Rooy</a>.<br>Built with <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>.
Powered by <a target=_blank rel="noopener noreferrer" href=http://www.github.com>GitHub</a></center></footer></body></html>